# Azure Databricks Insurance Pipeline

## ğŸš€ Contexte

Ce projet a pour objectif la conception dâ€™une **plateforme de Data Engineering complÃ¨te sur Microsoft Azure** autour dâ€™un dataset dâ€™assurance auto.

Lâ€™objectif est de transformer des donnÃ©es brutes vers des donnÃ©es analytiques prÃªtes Ã  lâ€™usage (BI ou future exploitation ML) en suivant une **architecture Medallion (Bronze / Silver / Gold)**.

Le dataset utilisÃ© est issu de Kaggle : *Insurance Claims* (58 592 lignes, 41 colonnes).

---
## ğŸ’¼ ProblÃ©matique mÃ©tier

Une compagnie dâ€™assurance souhaite :

- Centraliser ses donnÃ©es contrats et sinistres
- Nettoyer les donnÃ©es issues de diffÃ©rents systÃ¨mes
- Disposer dâ€™un modÃ¨le analytique fiable pour :
  - Analyse des primes
  - Analyse des risques
  - Analyse rÃ©gionale

---

## ğŸ— Architecture dâ€™ensemble

Le pipeline suit une approche structurÃ©e :
```
CSV (Raw)
   â†“
Bronze (Delta Raw)
   â†“
Silver (Cleaned)
   â†“
Gold (Star Schema)
   â†“
BI / Analytics
```
---

## âš¡ Optimisations mises en place

- Format Delta Lake pour performance et ACID
- Partitionnement des tables Gold
- Auto-termination du cluster
- Architecture modulaire Ã©volutive

---

---

## ğŸ­ Industrialisation & Bonnes Pratiques

- Architecture Medallion (sÃ©paration claire des couches)
- Format Delta Lake (ACID, performance, versioning)
- Orchestration centralisÃ©e via Databricks Job
- Planification automatique
- Gestion des dÃ©pendances et arrÃªt en cas dâ€™Ã©chec
- Optimisation des ressources cloud

---

## ğŸ“ Structure du repositor
```
azure-databricks-insurance-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ insurance_claims_sample.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_bronze_ingestion.py
â”‚ â”œâ”€â”€ 02_silver_transformation.py
â”‚ â”œâ”€â”€ 03_gold_modeling.py
â”‚ â””â”€â”€ 04_job_orchestration.md
â”‚
â”œâ”€â”€ pipeline/
â”‚ â””â”€â”€ databricks-job-config.json
â”‚
â”œâ”€â”€ screenshots/
â”‚ â”œâ”€â”€ adls_containers.png
â”‚ â”œâ”€â”€ databricks_cluster.png
â”‚ â”œâ”€â”€ job-architecture.png
â”‚
â””â”€â”€ README.md
```
---

## ğŸ“Œ Description des composants

### ğŸŸ« Bronze â€“ Ingestion des donnÃ©es

Le notebook `01_bronze_ingestion.py` lit le CSV source et Ã©crit les donnÃ©es brutes dans le container Bronze au format **Delta Lake**.

Objectif :  
ğŸ“ Conserver lâ€™Ã©tat source sans transformation.

---

### ğŸŸ¦ Silver â€“ Nettoyage et transformations

ExÃ©cutÃ© dans `02_silver_transformation.py`, ce notebook rÃ©alise :

- Conversion des colonnes `Yes` / `No` en boolÃ©ens
- Parsing des colonnes techniques :
  - `max_torque` â†’ `torque_nm`, `torque_rpm`
  - `max_power` â†’ `power_bhp`, `power_rpm`
- Nettoyage des colonnes inutiles
- Standardisation des formats

Objectif :  
ğŸ”¹ PrÃ©parer un jeu de donnÃ©es propre et cohÃ©rent pour la modÃ©lisation.

---

### ğŸŸ© Gold â€“ ModÃ¨le analytique

Le notebook `03_gold_modeling.py` gÃ©nÃ¨re un schÃ©ma en **Ã©toile** avec :

- `fact_policy`
- `dim_customer`
- `dim_vehicle`
- `dim_region`

Objectif :  
ğŸ“Š Construire des tables prÃªtes Ã  lâ€™usage pour BI ou exploration avancÃ©e.

---

## âš™ Orchestration & Planification

La dÃ©finition du job Databricks est disponible dans :
* `pipeline/databricks-job-config.json`

Le pipeline est orchestrÃ© via un **Job Azure Databricks planifiÃ© automatiquement**.

CaractÃ©ristiques :

- ExÃ©cution sÃ©quentielle : Bronze â†’ Silver â†’ Gold
- DÃ©pendances explicites entre tÃ¢ches
- Planification automatique via scheduler Databricks
- Historique complet des runs
- Monitoring intÃ©grÃ©
- Auto-termination du cluster pour optimisation des coÃ»ts

Cette configuration rapproche le projet dâ€™un environnement de production rÃ©el.

---

## ğŸ” Visualisation des composants (captures)

Les **captures dâ€™Ã©cran** sont disponibles dans le dossier :

ğŸ“ `screenshots/`

| Capture                       | Description                                                   |
|------------------------------|---------------------------------------------------------------|
| `adls_containers.png`         | Containers Bronze / Silver / Gold dans Azure Data Lake Gen2  |
| `databricks_cluster.png`      | Configuration du cluster Databricks                           |
| `job-architecture.png`      | l'image de l'exÃ©cution des diffÃ©rentes tasks                           |

---

## ğŸ§  Technologies utilisÃ©es

- â˜ï¸ Azure Data Lake Storage Gen2 (ADLS)
- ğŸ”¥ Azure Databricks (Spark & Delta Lake)
- ğŸ§ª Spark DataFrame API
- ğŸ“ Python / SQL
- ğŸ” Databricks Job (orchestration)
- ğŸ“Š (PrÃ©paration BI future)

---

## ğŸ¯ RÃ©sultats

- Pipeline Data Engineering complet et automatisÃ©
- Architecture Medallion mise en place
- DonnÃ©es structurÃ©es prÃªtes Ã  lâ€™usage
- Base solide pour BI ou Analytics

---

## ğŸ“ˆ Perspectives dâ€™Ã©volution

Voici des pistes dâ€™amÃ©lioration futures :

- IntÃ©gration dâ€™alertes en cas dâ€™Ã©chec
- Connecteur vers Databricks SQL Warehouse / Power BI
- ContrÃ´les de qualitÃ© automatisÃ©s
- Mise en place CI/CD via Azure DevOps
- SÃ©curisation via Azure Key Vault

---

## ğŸ§¾ Licence

Ce projet est sous licence MIT â€” libre Ã  utiliser et Ã  adapter.
