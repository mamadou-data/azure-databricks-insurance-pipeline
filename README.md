# Azure Databricks Insurance Pipeline

## ğŸš€ Contexte

Ce projet a pour objectif la conception dâ€™une **plateforme de Data Engineering complÃ¨te sur Microsoft Azure** autour dâ€™un dataset dâ€™assurance auto.

Lâ€™objectif est de transformer des donnÃ©es brutes vers des donnÃ©es analytiques prÃªtes Ã  lâ€™usage (BI ou future exploitation ML) en suivant une **architecture Medallion (Bronze / Silver / Gold)**.

Le dataset utilisÃ© est issu de Kaggle : *Insurance Claims* (58 592 lignes, 41 colonnes).

---

## ğŸ— Architecture dâ€™ensemble

Le pipeline suit une approche structurÃ©e :



---

## ğŸ“ Structure du repositor
```
azure-databricks-insurance-pipeline/
â”‚
â”œâ”€â”€ architecture/
â”‚ â”œâ”€â”€ architecture-diagram.png
â”‚ â””â”€â”€ architecture-description.md
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ insurance_claims_sample.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_bronze_ingestion.py
â”‚ â”œâ”€â”€ 02_silver_transformation.py
â”‚ â”œâ”€â”€ 03_gold_modeling.py
â”‚ â””â”€â”€ 04_job_orchestration.md
â”‚
â”œâ”€â”€ pipeline/
â”‚ â””â”€â”€ databricks-job-config.json
â”‚
â”œâ”€â”€ screenshots/
â”‚ â”œâ”€â”€ adls_containers.png
â”‚ â”œâ”€â”€ databricks_cluster.png
â”‚ â”œâ”€â”€ delta_tables.png
â”‚
â””â”€â”€ README.md
```
---

## ğŸ“Œ Description des composants

### ğŸŸ« Bronze â€“ Ingestion des donnÃ©es

Le notebook `01_bronze_ingestion.py` lit le CSV source et Ã©crit les donnÃ©es brutes dans le container Bronze au format **Delta Lake**.

Objectif :  
ğŸ“ Conserver lâ€™Ã©tat source sans transformation.

---

### ğŸŸ¦ Silver â€“ Nettoyage et transformations

ExÃ©cutÃ© dans `02_silver_transformation.py`, ce notebook rÃ©alise :

- Conversion des colonnes `Yes` / `No` en boolÃ©ens
- Parsing des colonnes techniques :
  - `max_torque` â†’ `torque_nm`, `torque_rpm`
  - `max_power` â†’ `power_bhp`, `power_rpm`
- Nettoyage des colonnes inutiles
- Standardisation des formats

Objectif :  
ğŸ”¹ PrÃ©parer un jeu de donnÃ©es propre et cohÃ©rent pour la modÃ©lisation.

---

### ğŸŸ© Gold â€“ ModÃ¨le analytique

Le notebook `03_gold_modeling.py` gÃ©nÃ¨re un schÃ©ma en **Ã©toile** avec :

- `fact_policy`
- `dim_customer`
- `dim_vehicle`
- `dim_region`

Objectif :  
ğŸ“Š Construire des tables prÃªtes Ã  lâ€™usage pour BI ou exploration avancÃ©e.

---

## âš™ Orchestration du Pipeline

La dÃ©finition du job Databricks (fichier JSON) est disponible dans :
* pipeline/databricks-job-config.json

Ce job exÃ©cute :

1. Bronze  
2. Silver  
3. Gold

de faÃ§on sÃ©quentielle, avec auto-termination du cluster.

---

## ğŸ” Visualisation des composants (captures)

Les **captures dâ€™Ã©cran** sont disponibles dans le dossier :

ğŸ“ `screenshots/`

| Capture                       | Description                                                   |
|------------------------------|---------------------------------------------------------------|
| `adls_containers.png`         | Containers Bronze / Silver / Gold dans Azure Data Lake Gen2  |
| `databricks_cluster.png`      | Configuration du cluster Databricks                           |
| `delta_tables.png`            | Tables Gold crÃ©Ã©es au format Delta                            |

---

## ğŸ§  Technologies utilisÃ©es

- â˜ï¸ Azure Data Lake Storage Gen2 (ADLS)
- ğŸ”¥ Azure Databricks (Spark & Delta Lake)
- ğŸ§ª Spark DataFrame API
- ğŸ“ Python / SQL
- ğŸ” Databricks Job (orchestration)
- ğŸ“Š (PrÃ©paration BI future)

---

## ğŸ¯ RÃ©sultats

- Pipeline Data Engineering complet et automatisÃ©
- Architecture Medallion mise en place
- DonnÃ©es structurÃ©es prÃªtes Ã  lâ€™usage
- Base solide pour BI ou Analytics

---

## ğŸ“ˆ Perspectives dâ€™Ã©volution

Voici des pistes dâ€™amÃ©lioration futures :

- Planification automatique via Databricks Scheduler
- IntÃ©gration dâ€™alertes en cas dâ€™Ã©chec
- Connecteur vers Databricks SQL Warehouse / Power BI
- ContrÃ´les de qualitÃ© automatisÃ©s
- Mise en place CI/CD via Azure DevOps
- SÃ©curisation via Azure Key Vault

---

## ğŸ§¾ Licence

Ce projet est sous licence MIT â€” libre Ã  utiliser et Ã  adapter.
